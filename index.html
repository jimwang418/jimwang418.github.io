<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Junxiang (Jim) Wang 王俊翔 </title> <meta name="author" content="Junxiang (Jim) Wang"> <meta name="description" content="Homepage for Junxiang Wang "> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/favicon.png?fdbcaa2de66f68f41b6835259816f0ec"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://jimwang418.github.io//"> <script src="/assets/js/theme.js?a81d82887dd692e91686b43de4542f18"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav sticky-bottom-footer"> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item active"> <a class="nav-link" href="/">about <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title"> Junxiang (Jim) Wang 王俊翔 </h1> <p class="desc">Robotics Researcher</p> </header> <article> <div class="profile float-right"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/profile_pic-480.webp 480w,/assets/img/profile_pic-800.webp 800w,/assets/img/profile_pic-1400.webp 1400w," type="image/webp" sizes="(min-width: 930px) 270.0px, (min-width: 576px) 30vw, 95vw"> <img src="/assets/img/profile_pic.jpg?77cdcfeef3850614fcccb3b0c3c9f805" class="img-fluid z-depth-1 rounded" width="100%" height="auto" alt="profile_pic.jpg" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div class="clearfix"> <p><strong>Hi! I’m Jim!</strong> I’m a first-year Ph.D. student at the <a href="https://www.ri.cmu.edu/" rel="external nofollow noopener" target="_blank">Robotics Institute</a> at <a href="https://www.cmu.edu/" rel="external nofollow noopener" target="_blank">Carnegie Mellon University</a>, advised by Prof. <a href="https://zackory.com/" rel="external nofollow noopener" target="_blank">Zackory Erickson</a>. Prior to joining RI, I obtained my B.S. in Mechanical Engineering and M.S.E. in Robotics from <a href="https://www.jhu.edu/" rel="external nofollow noopener" target="_blank">Johns Hopkins University</a>, where I was fortunate to work with Prof. <a href="https://smarts.lcsr.jhu.edu/people/peter-kazanzides/" rel="external nofollow noopener" target="_blank">Peter Kazanzides</a>, Prof. <a href="https://www.cs.jhu.edu/~cmhuang/" rel="external nofollow noopener" target="_blank">Chien-Ming Huang</a>, and Prof. <a href="https://amiro.lcsr.jhu.edu/iulianiordachita/" rel="external nofollow noopener" target="_blank">Iulian Iordachita</a>.</p> <p>I am broadly interested in human-robot interaction. Specifically, my work focuses on natural-language communication between humans and agents, as well as physical interactions between humans and robots.</p> <p>Contact: <code class="language-plaintext highlighter-rouge">junxiang [at] cmu [dot] edu</code></p> <div style="text-align: center;"> <a href="https://scholar.google.com/citations?user=K7Zzm7YAAAAJ/" rel="external nofollow noopener" target="_blank">Google Scholar <i class="ai ai-google-scholar"></i></a>   /   <a href="https://github.com/jimwang418" rel="external nofollow noopener" target="_blank">GitHub <i class="fab fa-github"></i></a>   /   <a href="https://twitter.com/wang_junxiang_" rel="external nofollow noopener" target="_blank">X <i class="fab fa-x-twitter"></i></a> </div> </div> <h2> <a href="/news/" style="color: inherit">news</a> </h2> <div class="news"> <div class="table-responsive"> <table class="table table-sm table-borderless"> <tr> <th scope="row" style="width: 20%">Feb 17, 2025</th> <td> dVRK digital twin was accepted to present at <a href="https://ismr.gatech.edu/2025/2025-welcome" rel="external nofollow noopener" target="_blank">ISMR 2025</a> in Atlanta! (<a href="https://github.com/LCSR-CIIS/dvrk_digital_twin_teleoperation" rel="external nofollow noopener" target="_blank">code <i class="fab fa-github"></i></a>, <a href="/assets/pdf/a_digital_twin_for_telesurgery_under_intermittent_communication.pdf">paper <i class="fas fa-file"></i></a>) </td> </tr> <tr> <th scope="row" style="width: 20%">Dec 03, 2024</th> <td> LLM-powered voice assistant was published in <a href="https://www.sciencedirect.com/journal/international-journal-of-human-computer-studies" rel="external nofollow noopener" target="_blank">IJHCS</a>! </td> </tr> <tr> <th scope="row" style="width: 20%">Jun 30, 2023</th> <td> Motion compensation for mobile PET imaging was accepted to present at <a href="https://2023.ieee-iros.org/" rel="external nofollow noopener" target="_blank">IROS 2023</a> in Detroit! </td> </tr> <tr> <th scope="row" style="width: 20%">Apr 15, 2022</th> <td> Motion measurement for PET imaging won Best Student Paper at <a href="https://robomed.gatech.edu/2022-welcome/" rel="external nofollow noopener" target="_blank">ISMR 2022</a> in Atlanta! </td> </tr> </table> </div> </div> <h2> <a href="/publications/" style="color: inherit">selected publications</a> </h2> <div class="publications"> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#7C1817"> <a href="http://arxiv.org/abs/2505.20537" rel="external nofollow noopener" target="_blank">Under Review</a> </abbr> <figure> <picture> <img src="/assets/img/publication_preview/cori.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="cori.png" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="wang2025cori" class="col-sm-8"> <div class="title">CoRI: Synthesizing Communication of Robot Intent for Physical Human-Robot Interaction</div> <div class="author"> <em>Junxiang Wang</em>, Emek Barış Küçüktabak, Rana Soltani Zarrin, and <a href="https://zackory.com/" rel="external nofollow noopener" target="_blank">Zackory Erickson</a> </div> <div class="periodical"> <em>arXiv preprint arXiv:2505.20537</em>, 2025 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2505.20537" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv <i class="ai ai-arxiv"></i></a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/assets/pdf/CoRI_synthesizing%20communication_of_robot_intent_for_physical_human-robot_interaction.pdf" class="btn btn-sm z-depth-0" role="button">PDF <i class="fas fa-file"></i></a> </div> <div class="abstract hidden"> <p>Clear communication of robot intent fosters transparency and interpretability in physical human-robot interaction (pHRI), particularly during assistive tasks involving direct human-robot contact. We introduce CoRI, a pipeline that automatically generates natural language communication of a robot’s upcoming actions directly from its motion plan and visual perception. Our pipeline first processes the robot’s image view to identify human poses and key environmental features. It then encodes the planned 3D spatial trajectory (including velocity and force) onto this view, visually grounding the path and its dynamics. CoRI queries a vision-language model with this visual representation to interpret the planned action within the visual context before generating concise, user-directed statements, without relying on task-specific information. Results from a user study involving robot-assisted feeding, bathing, and shaving tasks across two different robots indicate that CoRI leads to statistically significant difference in communication clarity compared to a baseline communication strategy. Specifically, CoRI effectively conveys not only the robot’s high-level intentions but also crucial details about its motion and any collaborative user action needed.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">wang2025cori</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{{CoRI}: Synthesizing Communication of Robot Intent for Physical Human-Robot Interaction}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Wang, Junxiang and K{\"u}{\c{c}}{\"u}ktabak, Emek Bar{\i}{\c{s}} and Zarrin, Rana Soltani and Erickson, Zackory}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{arXiv preprint arXiv:2505.20537}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#233265"> <a href="https://www.sciencedirect.com/journal/international-journal-of-human-computer-studies" rel="external nofollow noopener" target="_blank">IJHCS</a> </abbr> <figure> <picture> <img src="/assets/img/publication_preview/llmva.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="llmva.png" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="mahmood2025user" class="col-sm-8"> <div class="title">User interaction patterns and breakdowns in conversing with LLM-powered voice assistants</div> <div class="author"> <a href="https://amamamahmood.github.io/" rel="external nofollow noopener" target="_blank">Amama Mahmood</a>, <em>Junxiang Wang</em>, <a href="https://www.bingshengyao.com/" rel="external nofollow noopener" target="_blank">Bingsheng Yao</a>, <a href="https://www.dakuowang.com/" rel="external nofollow noopener" target="_blank">Dakuo Wang</a>, and <a href="https://www.cs.jhu.edu/~cmhuang/" rel="external nofollow noopener" target="_blank">Chien-Ming Huang</a> </div> <div class="periodical"> <em>International Journal of Human-Computer Studies</em>, 2025 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.1016/j.ijhcs.2024.103406" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> <a href="http://arxiv.org/abs/2309.13879" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv <i class="ai ai-arxiv"></i></a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="abstract hidden"> <p>Conventional Voice Assistants (VAs) rely on traditional language models to discern user intent and respond to their queries, leading to interactions that often lack a broader contextual understanding, an area in which Large Language Models (LLMs) excel. However, current LLMs are largely designed for text-based interactions, thus making it unclear how user interactions will evolve if their modality is changed to voice. In this work, we investigate whether LLMs can enrich VA interactions via an exploratory study with participants (N=20) using a ChatGPT-powered VA for three scenarios (medical self-diagnosis, creative planning, and discussion) with varied constraints, stakes, and objectivity. We observe that LLM-powered VA elicits richer interaction patterns that vary across tasks, showing its versatility. Notably, LLMs absorb the majority of VA intent recognition failures. We additionally discuss the potential of harnessing LLMs for more resilient and fluid user-VA interactions and provide design guidelines for tailoring LLMs for voice assistance.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">mahmood2025user</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{User interaction patterns and breakdowns in conversing with {LLM}-powered voice assistants}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Mahmood, Amama and Wang, Junxiang and Yao, Bingsheng and Wang, Dakuo and Huang, Chien-Ming}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{International Journal of Human-Computer Studies}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{195}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{103406}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Elsevier}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1016/j.ijhcs.2024.103406}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#8A7E55"> <a href="https://ismr.gatech.edu/2025/2025-welcome/" rel="external nofollow noopener" target="_blank">ISMR 2025</a> </abbr> <figure> <picture> <img src="/assets/img/publication_preview/dvrk_digital_twin.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="dvrk_digital_twin.png" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="wang2025digital" class="col-sm-8"> <div class="title">A digital twin for telesurgery under intermittent communication</div> <div class="author"> <em>Junxiang Wang<sup>*</sup></em>, Juan Antonio Barragan<sup>*</sup>, <a href="https://hisashiishida.github.io/" rel="external nofollow noopener" target="_blank">Hisashi Ishida<sup>*</sup></a>, Jingkai Guo, Yu-Chun Ku, and <a href="https://smarts.lcsr.jhu.edu/people/peter-kazanzides/" rel="external nofollow noopener" target="_blank">Peter Kazanzides</a> <i class="fa-solid fa-circle-info ml-1" data-toggle="popover" data-placement="top" data-html="true" data-content="*Equal contribution"> </i> </div> <div class="periodical"> <em>In 2025 International Symposium on Medical Robotics (ISMR)</em>, 2025 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.1109/ISMR67322.2025.11025988" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> <a href="http://arxiv.org/abs/2411.13449" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv <i class="ai ai-arxiv"></i></a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/assets/pdf/a_digital_twin_for_telesurgery_under_intermittent_communication.pdf" class="btn btn-sm z-depth-0" role="button">PDF <i class="fas fa-file"></i></a> <a href="https://github.com/LCSR-CIIS/dvrk_digital_twin_teleoperation" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code <i class="fab fa-github"></i></a> </div> <div class="abstract hidden"> <p>Telesurgery is an effective way to deliver service from expert surgeons to areas without immediate access to specialized resources. However, many of these areas, such as rural districts or battlefields, might be subject to different problems in communication, especially latency and intermittent periods of communication outage. This challenge motivates the use of a digital twin for the surgical system, where a simulation would mirror the robot hardware and surgical environment in the real world. The surgeon would then be able to interact with the digital twin during communication outage, followed by a recovery strategy on the real robot upon reestablishing communication. This paper builds the digital twin for the da Vinci surgical robot, with a buffering and replay strategy that reduces the mean task completion time by 23 % when compared to the baseline, for a peg transfer task subject to intermittent communication outage.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">wang2025digital</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{A digital twin for telesurgery under intermittent communication}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Wang, Junxiang and Barragan, Juan Antonio and Ishida, Hisashi and Guo, Jingkai and Ku, Yu-Chun and Kazanzides, Peter}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{2025 International Symposium on Medical Robotics (ISMR)}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{218--224}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span><span class="p">,</span>
  <span class="na">organization</span> <span class="p">=</span> <span class="s">{IEEE}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1109/ISMR67322.2025.11025988}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#7C1817"> <a href="http://arxiv.org/abs/2403.02421" rel="external nofollow noopener" target="_blank">Under Review</a> </abbr> <figure> <picture> <img src="/assets/img/publication_preview/old_adult_va.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="old_adult_va.png" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="mahmood2024situated" class="col-sm-8"> <div class="title">Situated Understanding of Older Adults’ Interactions with Voice Assistants: A Month-long In-home Study</div> <div class="author"> <a href="https://amamamahmood.github.io/" rel="external nofollow noopener" target="_blank">Amama Mahmood</a>, <em>Junxiang Wang</em>, and <a href="https://www.cs.jhu.edu/~cmhuang/" rel="external nofollow noopener" target="_blank">Chien-Ming Huang</a> </div> <div class="periodical"> <em>arXiv e-prints</em>, 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2403.02421" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv <i class="ai ai-arxiv"></i></a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="abstract hidden"> <p>Our work addresses the challenges older adults face with commercial Voice Assistants (VAs), notably in conversation breakdowns and error handling. Traditional methods of collecting user experiences-usage logs and post-hoc interviews-do not fully capture the intricacies of older adults’ interactions with VAs, particularly regarding their reactions to errors. To bridge this gap, we equipped 15 older adults’ homes with smart speakers integrated with custom audio recorders to collect "in-the-wild" audio interaction data for detailed error analysis. Recognizing the conversational limitations of current VAs, our study also explored the capabilities of Large Language Models (LLMs) to handle natural and imperfect text for improving VAs. Midway through our study, we deployed ChatGPT-powered VA to investigate its efficacy for older adults. Our research suggests leveraging vocal and verbal responses combined with LLMs’ contextual capabilities for enhanced error prevention and management in VAs, while proposing design considerations to align VA capabilities with older adults’ expectations.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">mahmood2024situated</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Situated Understanding of Older Adults' Interactions with Voice Assistants: A Month-long In-home Study}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Mahmood, Amama and Wang, Junxiang and Huang, Chien-Ming}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{arXiv e-prints}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{arXiv--2403}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> </div> <div class="social"> <div class="contact-icons"> <a href="mailto:%6A%75%6E%78%69%61%6E%67@%63%6D%75.%65%64%75" title="email"><i class="fa-solid fa-envelope"></i></a> <a href="https://github.com/jimwang418" title="GitHub" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-github"></i></a> <a href="https://www.linkedin.com/in/junxiang-jim-wang" title="LinkedIn" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-linkedin"></i></a> <a href="https://scholar.google.com/citations?user=K7Zzm7YAAAAJ" title="Google Scholar" rel="external nofollow noopener" target="_blank"><i class="ai ai-google-scholar"></i></a> <a href="https://twitter.com/wang_junxiang_" title="X" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-x-twitter"></i></a> </div> <div class="contact-note"></div> </div> </article> </div> </div> <footer class="sticky-bottom mt-5" role="contentinfo"> <div class="container"> © Copyright 2025 Junxiang (Jim) Wang. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js?a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> </body> </html>