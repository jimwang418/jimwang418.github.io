<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> publications | Junxiang (Jim) Wang 王俊翔 </title> <meta name="author" content="Junxiang (Jim) Wang"> <meta name="description" content="Homepage for Junxiang Wang "> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/favicon.png?fdbcaa2de66f68f41b6835259816f0ec"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://jimwang418.github.io//publications/"> <script src="/assets/js/theme.js?a81d82887dd692e91686b43de4542f18"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav sticky-bottom-footer"> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> Junxiang (Jim) Wang 王俊翔 </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item active"> <a class="nav-link" href="/publications/">publications <span class="sr-only">(current)</span> </a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">publications</h1> <p class="post-description"></p> </header> <article> <p><a href="https://scholar.google.com/citations?user=K7Zzm7YAAAAJ/" rel="external nofollow noopener" target="_blank">Google Scholar <i class="ai ai-google-scholar"></i></a></p> <div class="publications"> <h2 class="bibliography">2025</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#7C1817"> <a href="http://arxiv.org/abs/2505.20537" rel="external nofollow noopener" target="_blank">Under Review</a> </abbr> <figure> <picture> <img src="/assets/img/publication_preview/cori.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="cori.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="wang2025cori" class="col-sm-8"> <div class="title">CoRI: Synthesizing Communication of Robot Intent for Physical Human-Robot Interaction</div> <div class="author"> <em>Junxiang Wang</em>, Emek Barış Küçüktabak, Rana Soltani Zarrin, and <a href="https://zackory.com/" rel="external nofollow noopener" target="_blank">Zackory Erickson</a> </div> <div class="periodical"> <em>arXiv preprint arXiv:2505.20537</em>, 2025 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2505.20537" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv <i class="ai ai-arxiv"></i></a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/assets/pdf/CoRI_synthesizing%20communication_of_robot_intent_for_physical_human-robot_interaction.pdf" class="btn btn-sm z-depth-0" role="button">PDF <i class="fas fa-file"></i></a> </div> <div class="abstract hidden"> <p>Clear communication of robot intent fosters transparency and interpretability in physical human-robot interaction (pHRI), particularly during assistive tasks involving direct human-robot contact. We introduce CoRI, a pipeline that automatically generates natural language communication of a robot’s upcoming actions directly from its motion plan and visual perception. Our pipeline first processes the robot’s image view to identify human poses and key environmental features. It then encodes the planned 3D spatial trajectory (including velocity and force) onto this view, visually grounding the path and its dynamics. CoRI queries a vision-language model with this visual representation to interpret the planned action within the visual context before generating concise, user-directed statements, without relying on task-specific information. Results from a user study involving robot-assisted feeding, bathing, and shaving tasks across two different robots indicate that CoRI leads to statistically significant difference in communication clarity compared to a baseline communication strategy. Specifically, CoRI effectively conveys not only the robot’s high-level intentions but also crucial details about its motion and any collaborative user action needed.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">wang2025cori</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{{CoRI}: Synthesizing Communication of Robot Intent for Physical Human-Robot Interaction}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Wang, Junxiang and K{\"u}{\c{c}}{\"u}ktabak, Emek Bar{\i}{\c{s}} and Zarrin, Rana Soltani and Erickson, Zackory}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{arXiv preprint arXiv:2505.20537}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#233265"> <a href="https://www.sciencedirect.com/journal/international-journal-of-human-computer-studies" rel="external nofollow noopener" target="_blank">IJHCS</a> </abbr> <figure> <picture> <img src="/assets/img/publication_preview/llmva.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="llmva.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="mahmood2025user" class="col-sm-8"> <div class="title">User interaction patterns and breakdowns in conversing with LLM-powered voice assistants</div> <div class="author"> <a href="https://amamamahmood.github.io/" rel="external nofollow noopener" target="_blank">Amama Mahmood</a>, <em>Junxiang Wang</em>, <a href="https://www.bingshengyao.com/" rel="external nofollow noopener" target="_blank">Bingsheng Yao</a>, <a href="https://www.dakuowang.com/" rel="external nofollow noopener" target="_blank">Dakuo Wang</a>, and <a href="https://www.cs.jhu.edu/~cmhuang/" rel="external nofollow noopener" target="_blank">Chien-Ming Huang</a> </div> <div class="periodical"> <em>International Journal of Human-Computer Studies</em>, 2025 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.1016/j.ijhcs.2024.103406" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> <a href="http://arxiv.org/abs/2309.13879" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv <i class="ai ai-arxiv"></i></a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="abstract hidden"> <p>Conventional Voice Assistants (VAs) rely on traditional language models to discern user intent and respond to their queries, leading to interactions that often lack a broader contextual understanding, an area in which Large Language Models (LLMs) excel. However, current LLMs are largely designed for text-based interactions, thus making it unclear how user interactions will evolve if their modality is changed to voice. In this work, we investigate whether LLMs can enrich VA interactions via an exploratory study with participants (N=20) using a ChatGPT-powered VA for three scenarios (medical self-diagnosis, creative planning, and discussion) with varied constraints, stakes, and objectivity. We observe that LLM-powered VA elicits richer interaction patterns that vary across tasks, showing its versatility. Notably, LLMs absorb the majority of VA intent recognition failures. We additionally discuss the potential of harnessing LLMs for more resilient and fluid user-VA interactions and provide design guidelines for tailoring LLMs for voice assistance.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">mahmood2025user</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{User interaction patterns and breakdowns in conversing with {LLM}-powered voice assistants}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Mahmood, Amama and Wang, Junxiang and Yao, Bingsheng and Wang, Dakuo and Huang, Chien-Ming}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{International Journal of Human-Computer Studies}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{195}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{103406}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Elsevier}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1016/j.ijhcs.2024.103406}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#8A7E55"> <a href="https://ismr.gatech.edu/2025/2025-welcome/" rel="external nofollow noopener" target="_blank">ISMR 2025</a> </abbr> <figure> <picture> <img src="/assets/img/publication_preview/dvrk_digital_twin.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="dvrk_digital_twin.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="wang2025digital" class="col-sm-8"> <div class="title">A digital twin for telesurgery under intermittent communication</div> <div class="author"> <em>Junxiang Wang<sup>*</sup></em>, Juan Antonio Barragan<sup>*</sup>, <a href="https://hisashiishida.github.io/" rel="external nofollow noopener" target="_blank">Hisashi Ishida<sup>*</sup></a>, Jingkai Guo, Yu-Chun Ku, and <a href="https://smarts.lcsr.jhu.edu/people/peter-kazanzides/" rel="external nofollow noopener" target="_blank">Peter Kazanzides</a> <i class="fa-solid fa-circle-info ml-1" data-toggle="popover" data-placement="top" data-html="true" data-content="*Equal contribution"> </i> </div> <div class="periodical"> <em>In 2025 International Symposium on Medical Robotics (ISMR)</em>, 2025 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.1109/ISMR67322.2025.11025988" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> <a href="http://arxiv.org/abs/2411.13449" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv <i class="ai ai-arxiv"></i></a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="/assets/pdf/a_digital_twin_for_telesurgery_under_intermittent_communication.pdf" class="btn btn-sm z-depth-0" role="button">PDF <i class="fas fa-file"></i></a> <a href="https://github.com/LCSR-CIIS/dvrk_digital_twin_teleoperation" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code <i class="fab fa-github"></i></a> </div> <div class="abstract hidden"> <p>Telesurgery is an effective way to deliver service from expert surgeons to areas without immediate access to specialized resources. However, many of these areas, such as rural districts or battlefields, might be subject to different problems in communication, especially latency and intermittent periods of communication outage. This challenge motivates the use of a digital twin for the surgical system, where a simulation would mirror the robot hardware and surgical environment in the real world. The surgeon would then be able to interact with the digital twin during communication outage, followed by a recovery strategy on the real robot upon reestablishing communication. This paper builds the digital twin for the da Vinci surgical robot, with a buffering and replay strategy that reduces the mean task completion time by 23 % when compared to the baseline, for a peg transfer task subject to intermittent communication outage.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">wang2025digital</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{A digital twin for telesurgery under intermittent communication}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Wang, Junxiang and Barragan, Juan Antonio and Ishida, Hisashi and Guo, Jingkai and Ku, Yu-Chun and Kazanzides, Peter}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{2025 International Symposium on Medical Robotics (ISMR)}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{218--224}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span><span class="p">,</span>
  <span class="na">organization</span> <span class="p">=</span> <span class="s">{IEEE}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1109/ISMR67322.2025.11025988}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> <h2 class="bibliography">2024</h2> <ol class="bibliography"><li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#7C1817"> <a href="http://arxiv.org/abs/2403.02421" rel="external nofollow noopener" target="_blank">Under Review</a> </abbr> <figure> <picture> <img src="/assets/img/publication_preview/old_adult_va.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="old_adult_va.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="mahmood2024situated" class="col-sm-8"> <div class="title">Situated Understanding of Older Adults’ Interactions with Voice Assistants: A Month-long In-home Study</div> <div class="author"> <a href="https://amamamahmood.github.io/" rel="external nofollow noopener" target="_blank">Amama Mahmood</a>, <em>Junxiang Wang</em>, and <a href="https://www.cs.jhu.edu/~cmhuang/" rel="external nofollow noopener" target="_blank">Chien-Ming Huang</a> </div> <div class="periodical"> <em>arXiv e-prints</em>, 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="http://arxiv.org/abs/2403.02421" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv <i class="ai ai-arxiv"></i></a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="abstract hidden"> <p>Our work addresses the challenges older adults face with commercial Voice Assistants (VAs), notably in conversation breakdowns and error handling. Traditional methods of collecting user experiences-usage logs and post-hoc interviews-do not fully capture the intricacies of older adults’ interactions with VAs, particularly regarding their reactions to errors. To bridge this gap, we equipped 15 older adults’ homes with smart speakers integrated with custom audio recorders to collect "in-the-wild" audio interaction data for detailed error analysis. Recognizing the conversational limitations of current VAs, our study also explored the capabilities of Large Language Models (LLMs) to handle natural and imperfect text for improving VAs. Midway through our study, we deployed ChatGPT-powered VA to investigate its efficacy for older adults. Our research suggests leveraging vocal and verbal responses combined with LLMs’ contextual capabilities for enhanced error prevention and management in VAs, while proposing design considerations to align VA capabilities with older adults’ expectations.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">mahmood2024situated</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Situated Understanding of Older Adults' Interactions with Voice Assistants: A Month-long In-home Study}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Mahmood, Amama and Wang, Junxiang and Huang, Chien-Ming}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{arXiv e-prints}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{arXiv--2403}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li></ol> <h2 class="bibliography">2023</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#064445"> <a href="https://2023.ieee-iros.org/" rel="external nofollow noopener" target="_blank">IROS 2023</a> </abbr> <figure> <picture> <img src="/assets/img/publication_preview/motion_compensation.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="motion_compensation.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="wang2023method" class="col-sm-8"> <div class="title">Method for robotic motion compensation during PET imaging of mobile subjects</div> <div class="author"> <em>Junxiang Wang</em>, <a href="https://amiro.lcsr.jhu.edu/iulianiordachita/" rel="external nofollow noopener" target="_blank">Iulian Iordachita</a>, and <a href="https://smarts.lcsr.jhu.edu/people/peter-kazanzides/" rel="external nofollow noopener" target="_blank">Peter Kazanzides</a> </div> <div class="periodical"> <em>In 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</em>, 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.1109/IROS55552.2023.10341444" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> <a href="http://arxiv.org/abs/2311.17861" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv <i class="ai ai-arxiv"></i></a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="abstract hidden"> <p>Studies of the human brain during natural activities, such as locomotion, would benefit from the ability to image deep brain structures during these activities. While Positron Emission Tomography (PET) can image these structures, the bulk and weight of current scanners are not compatible with the desire for a wearable device. This has motivated the design of a robotic system to support a PET imaging system around the subject’s head and to move the system to accommodate natural motion. We report here the design and experimental evaluation of a prototype robotic system that senses motion of a subject’s head, using parallel string encoders connected between the robot-supported imaging ring and a helmet worn by the subject. This measurement is used to robotically move the imaging ring (coarse motion correction) and to compensate for residual motion during image reconstruction (fine motion correction). Minimization of latency and measurement error are the key design goals, respectively, for coarse and fine motion correction. The system is evaluated using recorded human head motions during locomotion, with a mock imaging system consisting of lasers and cameras, and is shown to provide an overall system latency of about 80 ms, which is sufficient for coarse motion correction and collision avoidance, as well as a measurement accuracy of about 0.5mm for fine motion correction.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">wang2023method</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Method for robotic motion compensation during {PET} imaging of mobile subjects}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Wang, Junxiang and Iordachita, Iulian and Kazanzides, Peter}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{4648--4654}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">organization</span> <span class="p">=</span> <span class="s">{IEEE}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1109/IROS55552.2023.10341444}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#052A62"> <a href="https://www.worldscientific.com/worldscinet/jmrr" rel="external nofollow noopener" target="_blank">JMRR</a> </abbr> <figure> <picture> <img src="/assets/img/publication_preview/motion_measurement.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="motion_measurement.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="wang2023calibration" class="col-sm-8"> <div class="title">Calibration and evaluation of a motion measurement system for PET imaging studies</div> <div class="author"> <em>Junxiang Wang</em>, Ti Wu, <a href="https://amiro.lcsr.jhu.edu/iulianiordachita/" rel="external nofollow noopener" target="_blank">Iulian Iordachita</a>, and <a href="https://smarts.lcsr.jhu.edu/people/peter-kazanzides/" rel="external nofollow noopener" target="_blank">Peter Kazanzides</a> </div> <div class="periodical"> <em>Journal of Medical Robotics Research</em>, 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.1142/S2424905X23400032" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> <a href="http://arxiv.org/abs/2311.18009" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv <i class="ai ai-arxiv"></i></a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="abstract hidden"> <p>Positron Emission Tomography (PET) enables functional imaging of deep brain structures, but the bulk and weight of current systems preclude their use during many natural human activities, such as locomotion. The proposed long-term solution is to construct a robotic system that can support an imaging system surrounding the subject’s head, and then move the system to accommodate natural motion. This requires a system to measure the motion of the head with respect to the imaging ring, for use by both the robotic system and the image reconstruction software. We report here the design, calibration, and experimental evaluation of a parallel string encoder mechanism for sensing this motion. Our results indicate that with kinematic calibration, the measurement system can achieve accuracy within 0.5mm, especially for small motions.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">wang2023calibration</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Calibration and evaluation of a motion measurement system for {PET} imaging studies}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Wang, Junxiang and Wu, Ti and Iordachita, Iulian and Kazanzides, Peter}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Journal of Medical Robotics Research}</span><span class="p">,</span>
  <span class="na">volume</span> <span class="p">=</span> <span class="s">{8}</span><span class="p">,</span>
  <span class="na">number</span> <span class="p">=</span> <span class="s">{01n02}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{2340003}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{World Scientific}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1142/S2424905X23400032}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> <h2 class="bibliography">2022</h2> <ol class="bibliography"><li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#8A7E55"> <a href="https://robomed.gatech.edu/2022-welcome/" rel="external nofollow noopener" target="_blank">ISMR 2022</a> </abbr> <figure> <picture> <img src="/assets/img/publication_preview/motion_measurement.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="motion_measurement.png" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="wang2022evaluation" class="col-sm-8"> <div class="title">Evaluation of a motion measurement system for PET imaging studies</div> <div class="author"> <em>Junxiang Wang</em>, Ti Wu, <a href="https://amiro.lcsr.jhu.edu/iulianiordachita/" rel="external nofollow noopener" target="_blank">Iulian Iordachita</a>, and <a href="https://smarts.lcsr.jhu.edu/people/peter-kazanzides/" rel="external nofollow noopener" target="_blank">Peter Kazanzides</a> </div> <div class="periodical"> <em>In 2022 International Symposium on Medical Robotics (ISMR)</em>, 2022 </div> <div class="periodical"> </div> <div class="links"> <a class="award btn btn-sm z-depth-0" role="button"> <i class="fa fa-award"></i>  Best Student Paper</a> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.1109/ISMR48347.2022.9807554" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> <a href="http://arxiv.org/abs/2311.17863" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv <i class="ai ai-arxiv"></i></a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="award hidden d-print-inline"> <p></p> <p>Awarded to the best paper with a student first-author (4%)</p> </div> <div class="abstract hidden"> <p>Positron Emission Tomography (PET) enables functional imaging of deep brain structures, but the bulk and weight of current systems preclude their use during many natural human activities, such as locomotion. The proposed long-term solution is to construct a robotic system that can support an imaging system surrounding the subject’s head, and then move the system to accommodate natural motion. This requires a system to measure the motion of the head with respect to the imaging ring, for use by both the robotic system and the image reconstruction software. We report here the design and experimental evaluation of a parallel string encoder mechanism for sensing this motion. Our preliminary results indicate that the measurement system may achieve accuracy within 0.5mm, especially for small motions, with improved accuracy possible through kinematic calibration.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">wang2022evaluation</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Evaluation of a motion measurement system for {PET} imaging studies}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Wang, Junxiang and Wu, Ti and Iordachita, Iulian and Kazanzides, Peter}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{2022 International Symposium on Medical Robotics (ISMR)}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{1--6}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2022}</span><span class="p">,</span>
  <span class="na">organization</span> <span class="p">=</span> <span class="s">{IEEE}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1109/ISMR48347.2022.9807554}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li></ol> </div> </article> </div> </div> <footer class="sticky-bottom mt-5" role="contentinfo"> <div class="container"> © Copyright 2025 Junxiang (Jim) Wang. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js?a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> </body> </html>